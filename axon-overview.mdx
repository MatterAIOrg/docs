---
title: "Models Overview"
description: "Overview of Axon AI Models - secure, production-ready AI models for general-purpose tasks, coding and deep-research."
og:title: "Axon Models By MatterAI | MatterAI Documentation"
og:description: "Overview of Axon AI Models By MatterAI - secure, production-ready AI models for general-purpose tasks, coding and deep-research."
og:image: "https://res.cloudinary.com/dxvbskvxm/image/upload/v1759829903/axon-models_n4c8t6.webp"
og:url: "https://docs.matterai.so/axon-models"
---

<Frame>
  <img
    src="https://res.cloudinary.com/dxvbskvxm/image/upload/v1759829903/axon-models_n4c8t6.webp"
    className="block"
  />
</Frame>

**Axon offers secure, production-ready AI models for both code generation and general-purpose tasks, leveraging State-of-the-Art Deep Reasoning powered by Interleaved Reasoning [https://arxiv.org/abs/2505.19640](https://arxiv.org/abs/2505.19640) and State Machines**

## Benchmarks

<Frame>
  <img src="/images/axon/axon-benchmarks.png" className="block" />
</Frame>

## Architecture

<Note>
  Axon Models are based on open source models from Qwen 3 family, fine tuned on
  our proprietary dataset and upgraded with deep reasoning and state machine
  capabilities.
</Note>

### Mixture of Experts (MoE) architecture

MoE is a technique that allows the model to dynamically select the expert model to use for a given input.

This architecture enables engineering teams to build more scalable and efficient systems by routing tasks to specialized experts, reducing computational overhead while maintaining high performance across diverse workloads.

## What makes Axon different?

- **Deep Reasoning**: Our SOTA Deep Reasoner generates a detailed reasoning process for your requests, detects what needs to be done and how to do it, ensuring all the context is considered and the best possible solution is provided.
- **State Machines**: Our SOTA State Machine uses temporal memories to remember your continued flow of usage on what has accomplished and what needs to be completed next.

## Deep Reasoner

Our State-of-the-Art Deep Reasoner generates a detailed reasoning process for your requests, detects what needs to be done and how to do it, ensuring all the context is considered and the best possible solution is provided.

<Note>
  The reasoning capabilities are derived from research presented in [Deep Reasoning with Large Language Models](https://arxiv.org/abs/2505.19640).
</Note>

- **Multi-sources causal graph traversal** for inferencing across heterogeneous data sources, enabling root-cause analysis and counterfactual reasoning.
- **Dynamic symbolic grounding** via contextual ontologies to map abstract concepts into actionable knowledge representations in real time.
- **Probabilistic logic synthesis** with uncertainty quantification to evaluate solution optimality under incomplete or ambiguous input conditions.
- **Hierarchical attention over structured memory** to maintain long-range dependencies during complex, multi-step problem decomposition.
- **Meta-cognitive feedback loops** that refine internal heuristics based on outcome validation, improving future reasoning trajectories.
- **Real-time web search integration** with federated query optimization across multiple search providers for comprehensive knowledge retrieval.
- **Adaptive web content parsing** using semantic-aware scrapers that extract structured data from dynamic web sources while respecting rate limits and ToS.

<Frame>
  <img src="/images/axon/axon-deep-reasoner.png" className="block" />
</Frame>

## State Machine

Our State-of-the-Art State Machine Engine uses temporal memories to remember your continued flow of usage on what has accomplished and what needs to be completed next.

- **Hierarchical semi-Markov decision processes** (HSMDPs) for modeling variable-duration states and adaptive task sequencing.
- **Distributed state persistence with vector-clock reconciliation** to ensure consistency across asynchronous, concurrent user sessions.
- **Reinforcement learning-driven transition policies** that optimize long-term user goal completion over immediate action rewards.
- **Temporal difference learning over latent state embeddings** to predict and pre-fetch likely next states for zero-latency transitions.
- **Context-sensitive state compression** using learned subroutines to reduce combinatorial state explosion while preserving semantic fidelity.

<Frame>
  <img src="/images/axon/axon-state-machine.png" className="block" />
</Frame>

## Model Family

<CardGroup cols={3}>
  <Card
    img="https://res.cloudinary.com/dxvbskvxm/image/upload/v1759829902/axon_mphl2c.webp"
    title="Axon 1"
    icon="brain-circuit"
    href="/axon"
  >
    General Purpose Model for high-effort day to day tasks
  </Card>
  <Card
    img="https://res.cloudinary.com/dxvbskvxm/image/upload/v1760447490/axon-mini_jjac8i.webp"
    title="Axon Mini 1"
    icon="microchip"
    href="/axon-mini"
  >
    General Purpose Model for low-effort day to day tasks
  </Card>
  <Card
    img="https://res.cloudinary.com/dxvbskvxm/image/upload/v1759829903/axon-code_kfrxe2.webp"
    title="Axon Code 1"
    icon="laptop-code"
    href="/axon-code"
  >
    Code Generation Model for high-effort coding tasks
  </Card>
</CardGroup>

## Getting Started

### Get API Key

<CardGroup cols={2}>
  <Card title="Get API Key" icon="key" href="/axon-ai/api-keys">
    Generate a new API key
  </Card>
</CardGroup>

### API & SDK Integration

<CodeGroup>

```bash cURL
curl --request POST \
  --url https://api.matterai.so/v1/chat/completions \
  --header 'Content-Type: application/json' \
  --header 'Authorization: Bearer MATTER_API_KEY' \
  --data '{
  "model": "{{MODEL_ID}}",
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful assistant."
    },
    {
      "role": "user",
      "content": "What is Rust?"
    }
  ],
  "stream": false,
  "max_tokens": 1000,
  "reasoning": {
    "effort": "high",
    "summary": "none"
  },
  "response_format": {
    "type": "text"
  },
  "temperature": 0,
  "top_p": 1
}'
```

```javascript OpenAI NodeJS SDK
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: "MATTER_API_KEY",
  baseURL: "https://api.matterai.so/v1",
});

async function main() {
  const response = await openai.chat.completions.create({
    model: "{{MODEL_ID}}",
    messages: [
      {
        role: "system",
        content: "You are a helpful assistant.",
      },
      {
        role: "user",
        content: "What is Rust?",
      },
    ],
    stream: false,
    max_tokens: 1000,
    reasoning: {
      effort: "high",
      summary: "none",
    },
    response_format: {
      type: "text",
    },
    temperature: 0,
    top_p: 1,
  });

  console.log(response.choices[0].message.content);
}

main();
```

```python OpenAI Python SDK
from openai import OpenAI

client = OpenAI(
  api_key='MATTER_API_KEY',
  base_url='https://api.matterai.so/v1'
)

response = client.chat.completions.create(
  model='{{MODEL_ID}}',
  messages=[
    {
      'role': 'system',
      'content': 'You are a helpful assistant.'
    },
    {
      'role': 'user',
      'content': 'What is Rust?'
    }
  ],
  stream=False,
  max_tokens=1000,
  reasoning={
    'effort': 'high',
    'summary': 'none'
  },
  response_format={
    'type': 'text'
  },
  temperature=0,
  top_p=1
)

print(response.choices[0].message.content)
```

</CodeGroup>

# Data Privacy

<Note>
  MatterAI never trains on your codebase, all data is temporary and deleted
  automatically.
</Note>
